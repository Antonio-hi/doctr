

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>doctr.models &mdash; doctr 0.1.2a0-git documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/mindee.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="doctr.utils" href="utils.html" />
    <link rel="prev" title="doctr.documents" href="documents.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> doctr
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installing.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installing.html#via-python-package">Via Python Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="installing.html#via-git">Via Git</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="datasets.html">doctr.datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="datasets.html#available-datasets">Available Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html#supported-vocabs">Supported Vocabs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="documents.html">doctr.documents</a><ul>
<li class="toctree-l2"><a class="reference internal" href="documents.html#document-structure">Document structure</a><ul>
<li class="toctree-l3"><a class="reference internal" href="documents.html#word">Word</a></li>
<li class="toctree-l3"><a class="reference internal" href="documents.html#line">Line</a></li>
<li class="toctree-l3"><a class="reference internal" href="documents.html#artefact">Artefact</a></li>
<li class="toctree-l3"><a class="reference internal" href="documents.html#block">Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="documents.html#page">Page</a></li>
<li class="toctree-l3"><a class="reference internal" href="documents.html#document">Document</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="documents.html#file-reading">File reading</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">doctr.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#text-detection">Text Detection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pre-processing-for-detection">Pre-processing for detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="#detection-models">Detection models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#post-processing-detections">Post-processing detections</a></li>
<li class="toctree-l3"><a class="reference internal" href="#detection-predictors">Detection predictors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#text-recognition">Text Recognition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pre-processing-for-recognition">Pre-processing for recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recognition-models">Recognition models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#post-processing-outputs">Post-processing outputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recognition-predictors">Recognition predictors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#end-to-end-ocr">End-to-End OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#two-stage-approaches">Two-stage approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-export">Model export</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">doctr.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="utils.html#visualization">Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html#task-evaluation">Task evaluation</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">doctr</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>doctr.models</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/models.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="doctr-models">
<h1>doctr.models<a class="headerlink" href="#doctr-models" title="Permalink to this headline">¶</a></h1>
<p>The full Optical Character Recognition task can be seen as two consecutive tasks: text detection and text recognition.
Either performed at once or separately, to each task corresponds a type of deep learning architecture.</p>
<p>For a given task, DocTR provides a Predictor, which is composed of 3 components:</p>
<ul class="simple">
<li><p>PreProcessor: a module in charge of making inputs directly usable by the TensorFlow model.</p></li>
<li><p>Model: a deep learning model, implemented with TensorFlow backend.</p></li>
<li><p>PostProcessor: making model outputs structured and reusable.</p></li>
</ul>
<div class="section" id="text-detection">
<h2>Text Detection<a class="headerlink" href="#text-detection" title="Permalink to this headline">¶</a></h2>
<p>Localizing text elements in images</p>
<table class="colwidths-given docutils align-default" id="id2">
<caption><span class="caption-text">Text detection model zoo</span><a class="headerlink" href="#id2" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 18%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Input shape</p></th>
<th class="head"><p># params</p></th>
<th class="head"><p>Recall</p></th>
<th class="head"><p>Precision</p></th>
<th class="head"><p>FPS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>db_resnet50</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td></td>
<td><p>0.775</p></td>
<td><p>0.856</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>All text detection models above have been evaluated using both the training and evaluation sets of FUNSD (cf. <a class="reference internal" href="datasets.html#datasets"><span class="std std-ref">Available Datasets</span></a>).
Explanations about the metrics being used are available in <a class="reference internal" href="utils.html#metrics"><span class="std std-ref">Task evaluation</span></a>.</p>
<p><em>Disclaimer: both subsets combine have 199 pages which might not be representative enough of the model capabilities</em></p>
<div class="section" id="pre-processing-for-detection">
<h3>Pre-processing for detection<a class="headerlink" href="#pre-processing-for-detection" title="Permalink to this headline">¶</a></h3>
<p>In DocTR, the pre-processing scheme for detection is the following:</p>
<ol class="arabic simple">
<li><p>resize each input image to the target size (bilinear interpolation by default) with potential deformation.</p></li>
<li><p>batch images together</p></li>
<li><p>normalize the batch using the training data statistics</p></li>
</ol>
</div>
<div class="section" id="detection-models">
<h3>Detection models<a class="headerlink" href="#detection-models" title="Permalink to this headline">¶</a></h3>
<p>Models expect a TensorFlow tensor as input and produces one in return. DocTR includes implementations and pretrained versions of the following models:</p>
<dl class="py function">
<dt id="doctr.models.detection.db_resnet50">
<code class="sig-prename descclassname">doctr.models.detection.</code><code class="sig-name descname">db_resnet50</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; doctr.models.detection.differentiable_binarization.DBNet<a class="reference internal" href="_modules/doctr/models/detection/differentiable_binarization.html#db_resnet50"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#doctr.models.detection.db_resnet50" title="Permalink to this definition">¶</a></dt>
<dd><p>DBNet as described in <a class="reference external" href="https://arxiv.org/pdf/1911.08947.pdf">“Real-time Scene Text Detection with Differentiable Binarization”</a>, using a ResNet-50 backbone.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">db_resnet50</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">db_resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>pretrained</strong> (<em>bool</em>) – If True, returns a model pre-trained on our text detection dataset</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>text detection architecture</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="post-processing-detections">
<h3>Post-processing detections<a class="headerlink" href="#post-processing-detections" title="Permalink to this headline">¶</a></h3>
<p>The purpose of this block is to turn the model output (binary segmentation map for instance), into a set of bounding boxes.</p>
</div>
<div class="section" id="detection-predictors">
<h3>Detection predictors<a class="headerlink" href="#detection-predictors" title="Permalink to this headline">¶</a></h3>
<p>Combining the right components around a given architecture for easier usage, predictors lets you pass numpy images as inputs and return structured information.</p>
<dl class="py function">
<dt id="doctr.models.detection.detection_predictor">
<code class="sig-prename descclassname">doctr.models.detection.</code><code class="sig-name descname">detection_predictor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">arch</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'db_resnet50'</span></em>, <em class="sig-param"><span class="n">pretrained</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; doctr.models.detection.core.DetectionPredictor<a class="reference internal" href="_modules/doctr/models/detection/zoo.html#detection_predictor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#doctr.models.detection.detection_predictor" title="Permalink to this definition">¶</a></dt>
<dd><p>Text detection architecture.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">detection_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">detection_predictor</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_page</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">input_page</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>arch</strong> – name of the architecture to use (‘db_resnet50’)</p></li>
<li><p><strong>pretrained</strong> – If True, returns a model pre-trained on our text detection dataset</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Detection predictor</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="text-recognition">
<h2>Text Recognition<a class="headerlink" href="#text-recognition" title="Permalink to this headline">¶</a></h2>
<p>Identifying strings in images</p>
<table class="colwidths-given docutils align-default" id="id3">
<caption><span class="caption-text">Text recognition model zoo</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 27%" />
<col style="width: 27%" />
<col style="width: 20%" />
<col style="width: 13%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Input shape</p></th>
<th class="head"><p># params</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>FPS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>crnn_vgg16_bn</p></td>
<td><p>(32, 128, 3)</p></td>
<td></td>
<td><p>0.860</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sar_vgg16_bn</p></td>
<td><p>(32, 128, 3)</p></td>
<td></td>
<td><p>0.862</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>crnn_resnet31</p></td>
<td><p>(32, 128, 3)</p></td>
<td></td>
<td><p>0.863</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sar_resnet31</p></td>
<td><p>(32, 128, 3)</p></td>
<td></td>
<td><p>0.863</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>All text recognition models above have been evaluated using both the training and evaluation sets of FUNSD (cf. <a class="reference internal" href="datasets.html#datasets"><span class="std std-ref">Available Datasets</span></a>).
Explanations about the metrics being used are available in <a class="reference internal" href="utils.html#metrics"><span class="std std-ref">Task evaluation</span></a>.</p>
<p><em>Disclaimer: both subsets combine have 30595 word-level crops which might not be representative enough of the model capabilities</em></p>
<div class="section" id="pre-processing-for-recognition">
<h3>Pre-processing for recognition<a class="headerlink" href="#pre-processing-for-recognition" title="Permalink to this headline">¶</a></h3>
<p>In DocTR, the pre-processing scheme for recognition is the following:</p>
<ol class="arabic simple">
<li><p>resize each input image to the target size (bilinear interpolation by default) without deformation.</p></li>
<li><p>pad the image to the target size (with zeros by default)</p></li>
<li><p>batch images together</p></li>
<li><p>normalize the batch using the training data statistics</p></li>
</ol>
</div>
<div class="section" id="recognition-models">
<h3>Recognition models<a class="headerlink" href="#recognition-models" title="Permalink to this headline">¶</a></h3>
<p>Models expect a TensorFlow tensor as input and produces one in return. DocTR includes implementations and pretrained versions of the following models:</p>
<dl class="py function">
<dt id="doctr.models.recognition.crnn_vgg16_bn">
<code class="sig-prename descclassname">doctr.models.recognition.</code><code class="sig-name descname">crnn_vgg16_bn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; doctr.models.recognition.crnn.CRNN<a class="reference internal" href="_modules/doctr/models/recognition/crnn.html#crnn_vgg16_bn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#doctr.models.recognition.crnn_vgg16_bn" title="Permalink to this definition">¶</a></dt>
<dd><p>CRNN with a VGG-16 backbone as described in <a class="reference external" href="https://arxiv.org/pdf/1507.05717.pdf">“An End-to-End Trainable Neural Network for Image-based
Sequence Recognition and Its Application to Scene Text Recognition”</a>.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">crnn_vgg16_bn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">crnn_vgg16_bn</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>pretrained</strong> (<em>bool</em>) – If True, returns a model pre-trained on our text recognition dataset</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>text recognition architecture</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="doctr.models.recognition.sar_vgg16_bn">
<code class="sig-prename descclassname">doctr.models.recognition.</code><code class="sig-name descname">sar_vgg16_bn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; doctr.models.recognition.sar.SAR<a class="reference internal" href="_modules/doctr/models/recognition/sar.html#sar_vgg16_bn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#doctr.models.recognition.sar_vgg16_bn" title="Permalink to this definition">¶</a></dt>
<dd><p>SAR with a VGG16 feature extractor as described in <a class="reference external" href="https://arxiv.org/pdf/1811.00751.pdf">“Show, Attend and Read:A Simple and Strong
Baseline for Irregular Text Recognition”</a>.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">sar_vgg16_bn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">sar_vgg16_bn</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>pretrained</strong> (<em>bool</em>) – If True, returns a model pre-trained on our text recognition dataset</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>text recognition architecture</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="doctr.models.recognition.sar_resnet31">
<code class="sig-prename descclassname">doctr.models.recognition.</code><code class="sig-name descname">sar_resnet31</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; doctr.models.recognition.sar.SAR<a class="reference internal" href="_modules/doctr/models/recognition/sar.html#sar_resnet31"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#doctr.models.recognition.sar_resnet31" title="Permalink to this definition">¶</a></dt>
<dd><p>SAR with a resnet-31 feature extractor as described in <a class="reference external" href="https://arxiv.org/pdf/1811.00751.pdf">“Show, Attend and Read:A Simple and Strong
Baseline for Irregular Text Recognition”</a>.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">sar_resnet31</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">sar_resnet31</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>pretrained</strong> (<em>bool</em>) – If True, returns a model pre-trained on our text recognition dataset</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>text recognition architecture</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="post-processing-outputs">
<h3>Post-processing outputs<a class="headerlink" href="#post-processing-outputs" title="Permalink to this headline">¶</a></h3>
<p>The purpose of this block is to turn the model output (symbol classification for the sequence), into a set of strings.</p>
</div>
<div class="section" id="recognition-predictors">
<h3>Recognition predictors<a class="headerlink" href="#recognition-predictors" title="Permalink to this headline">¶</a></h3>
<p>Combining the right components around a given architecture for easier usage.</p>
<dl class="py function">
<dt id="doctr.models.recognition.recognition_predictor">
<code class="sig-prename descclassname">doctr.models.recognition.</code><code class="sig-name descname">recognition_predictor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">arch</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'crnn_vgg16_bn'</span></em>, <em class="sig-param"><span class="n">pretrained</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; doctr.models.recognition.core.RecognitionPredictor<a class="reference internal" href="_modules/doctr/models/recognition/zoo.html#recognition_predictor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#doctr.models.recognition.recognition_predictor" title="Permalink to this definition">¶</a></dt>
<dd><p>Text recognition architecture.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">recognition_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">recognition_predictor</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_page</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">input_page</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>arch</strong> – name of the architecture to use (‘crnn_vgg16_bn’, ‘crnn_resnet31’, ‘sar_vgg16_bn’, ‘sar_resnet31’)</p></li>
<li><p><strong>pretrained</strong> – If True, returns a model pre-trained on our text recognition dataset</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Recognition predictor</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="end-to-end-ocr">
<h2>End-to-End OCR<a class="headerlink" href="#end-to-end-ocr" title="Permalink to this headline">¶</a></h2>
<p>Predictors that localize and identify text elements in images</p>
<table class="colwidths-given docutils align-default" id="id4">
<caption><span class="caption-text">end-to-end model zoo</span><a class="headerlink" href="#id4" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 18%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Input shape</p></th>
<th class="head"><p># params</p></th>
<th class="head"><p>Recall</p></th>
<th class="head"><p>Precision</p></th>
<th class="head"><p>FPS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>db_resnet50 + crnn_vgg16_bn</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td></td>
<td><p>0.652</p></td>
<td><p>0.721</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + sar_vgg16_bn</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td></td>
<td><p>0.653</p></td>
<td><p>0.721</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>db_resnet50 + crnn_resnet31</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td></td>
<td><p>0.665</p></td>
<td><p>0.735</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + sar_resnet31</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td></td>
<td><p>0.665</p></td>
<td><p>0.735</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>google vision</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td></td>
<td><p>0.595</p></td>
<td><p>0.625</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>aws textract</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td></td>
<td><p>0.781</p></td>
<td><p>0.830</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>All OCR models above have been evaluated using both the training and evaluation sets of FUNSD (cf. <a class="reference internal" href="datasets.html#datasets"><span class="std std-ref">Available Datasets</span></a>).
Explanations about the metrics being used are available in <a class="reference internal" href="utils.html#metrics"><span class="std std-ref">Task evaluation</span></a>.</p>
<p><em>Disclaimer: both subsets combine have 199 pages which might not be representative enough of the model capabilities</em></p>
<div class="section" id="two-stage-approaches">
<h3>Two-stage approaches<a class="headerlink" href="#two-stage-approaches" title="Permalink to this headline">¶</a></h3>
<p>Those architectures involve one stage of text detection, and one stage of text recognition. The text detection will be used to produces cropped images that will be passed into the text recognition block.</p>
<dl class="py function">
<dt id="doctr.models.zoo.ocr_predictor">
<code class="sig-prename descclassname">doctr.models.zoo.</code><code class="sig-name descname">ocr_predictor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">det_arch</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'db_resnet50'</span></em>, <em class="sig-param"><span class="n">reco_arch</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'crnn_vgg16_bn'</span></em>, <em class="sig-param"><span class="n">pretrained</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; doctr.models.core.OCRPredictor<a class="reference internal" href="_modules/doctr/models/zoo.html#ocr_predictor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#doctr.models.zoo.ocr_predictor" title="Permalink to this definition">¶</a></dt>
<dd><p>End-to-end OCR architecture using one model for localization, and another for text recognition.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">ocr_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ocr_predictor</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_page</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">input_page</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>arch</strong> – name of the architecture to use (‘db_sar_vgg’, ‘db_sar_resnet’, ‘db_crnn_vgg’, ‘db_crnn_resnet’)</p></li>
<li><p><strong>pretrained</strong> – If True, returns a model pre-trained on our OCR dataset</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>OCR predictor</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="model-export">
<h2>Model export<a class="headerlink" href="#model-export" title="Permalink to this headline">¶</a></h2>
<p>Utility functions to make the most of document analysis models.</p>
<dl class="py function">
<dt id="doctr.models.export.convert_to_tflite">
<code class="sig-prename descclassname">doctr.models.export.</code><code class="sig-name descname">convert_to_tflite</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tf_model</span><span class="p">:</span> <span class="n">tensorflow.python.keras.engine.training.Model</span></em><span class="sig-paren">)</span> &#x2192; bytes<a class="reference internal" href="_modules/doctr/models/export.html#convert_to_tflite"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#doctr.models.export.convert_to_tflite" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a model to TFLite format</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">convert_to_tflite</span><span class="p">,</span> <span class="n">conv_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">conv_sequence</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">serialized_model</span> <span class="o">=</span> <span class="n">convert_to_tflite</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tf_model</strong> – a keras model</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bytes</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="doctr.models.export.convert_to_fp16">
<code class="sig-prename descclassname">doctr.models.export.</code><code class="sig-name descname">convert_to_fp16</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tf_model</span><span class="p">:</span> <span class="n">tensorflow.python.keras.engine.training.Model</span></em><span class="sig-paren">)</span> &#x2192; bytes<a class="reference internal" href="_modules/doctr/models/export.html#convert_to_fp16"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#doctr.models.export.convert_to_fp16" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a model to half precision</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">convert_to_fp16</span><span class="p">,</span> <span class="n">conv_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">conv_sequence</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">serialized_model</span> <span class="o">=</span> <span class="n">convert_to_fp16</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tf_model</strong> – a keras model</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the serialized FP16 model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bytes</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="doctr.models.export.quantize_model">
<code class="sig-prename descclassname">doctr.models.export.</code><code class="sig-name descname">quantize_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tf_model</span><span class="p">:</span> <span class="n">tensorflow.python.keras.engine.training.Model</span></em>, <em class="sig-param"><span class="n">input_shape</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>int<span class="p">, </span>int<span class="p">, </span>int<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; bytes<a class="reference internal" href="_modules/doctr/models/export.html#quantize_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#doctr.models.export.quantize_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantize a Tensorflow model</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">quantize_model</span><span class="p">,</span> <span class="n">conv_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">conv_sequence</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">serialized_model</span> <span class="o">=</span> <span class="n">quantize_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tf_model</strong> – a keras model</p></li>
<li><p><strong>input_shape</strong> – shape of the expected input tensor (excluding batch dimension) with channel last order</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the serialized quantized model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bytes</p>
</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="utils.html" class="btn btn-neutral float-right" title="doctr.utils" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="documents.html" class="btn btn-neutral float-left" title="doctr.documents" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, Mindee

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
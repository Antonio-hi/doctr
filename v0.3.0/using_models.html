

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Choosing the right model &mdash; doctr 0.4.1a0-git documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/mindee.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Preparing your model for inference" href="using_model_export.html" />
    <link rel="prev" title="Installation" href="installing.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> doctr
          

          
            
            <img src="_static/Logo-docTR-white.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installing.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installing.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="installing.html#via-python-package">Via Python Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="installing.html#via-git">Via Git</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Using docTR</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Choosing the right model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#text-detection">Text Detection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#available-architectures">Available architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="#detection-predictors">Detection predictors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#text-recognition">Text Recognition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Available architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recognition-predictors">Recognition predictors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#end-to-end-ocr">End-to-End OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">Available architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="#two-stage-approaches">Two-stage approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="#what-should-i-do-with-the-output">What should I do with the output?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="using_model_export.html">Preparing your model for inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="using_model_export.html#model-compression">Model compression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="using_model_export.html#tensorflow-lite">TensorFlow Lite</a></li>
<li class="toctree-l3"><a class="reference internal" href="using_model_export.html#half-precision">Half-precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="using_model_export.html#post-training-quantization">Post-training quantization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="using_model_export.html#using-savedmodel">Using SavedModel</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">doctr.datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="datasets.html#available-datasets">Available Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html#data-loading">Data Loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html#supported-vocabs">Supported Vocabs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="io.html">doctr.io</a><ul>
<li class="toctree-l2"><a class="reference internal" href="io.html#document-structure">Document structure</a><ul>
<li class="toctree-l3"><a class="reference internal" href="io.html#word">Word</a></li>
<li class="toctree-l3"><a class="reference internal" href="io.html#line">Line</a></li>
<li class="toctree-l3"><a class="reference internal" href="io.html#artefact">Artefact</a></li>
<li class="toctree-l3"><a class="reference internal" href="io.html#block">Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="io.html#page">Page</a></li>
<li class="toctree-l3"><a class="reference internal" href="io.html#document">Document</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="io.html#file-reading">File reading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="models.html">doctr.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="models.html#doctr-models-backbones">doctr.models.backbones</a></li>
<li class="toctree-l2"><a class="reference internal" href="models.html#doctr-models-detection">doctr.models.detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="models.html#doctr-models-recognition">doctr.models.recognition</a></li>
<li class="toctree-l2"><a class="reference internal" href="models.html#doctr-models-zoo">doctr.models.zoo</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="transforms.html">doctr.transforms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transforms.html#supported-transformations">Supported transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="transforms.html#composing-transformations">Composing transformations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">doctr.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="utils.html#visualization">Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html#task-evaluation">Task evaluation</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-4-0-2021-10-01">v0.4.0 (2021-10-01)</a></li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-3-1-2021-08-27">v0.3.1 (2021-08-27)</a></li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-3-0-2021-07-02">v0.3.0 (2021-07-02)</a></li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-2-1-2021-05-28">v0.2.1 (2021-05-28)</a></li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-2-0-2021-05-11">v0.2.0 (2021-05-11)</a></li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-1-1-2021-03-18">v0.1.1 (2021-03-18)</a></li>
<li class="toctree-l2"><a class="reference internal" href="changelog.html#v0-1-0-2021-03-05">v0.1.0 (2021-03-05)</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">doctr</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Choosing the right model</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/using_models.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="choosing-the-right-model">
<h1>Choosing the right model<a class="headerlink" href="#choosing-the-right-model" title="Permalink to this headline">¶</a></h1>
<p>The full Optical Character Recognition task can be seen as two consecutive tasks: text detection and text recognition.
Either performed at once or separately, to each task corresponds a type of deep learning architecture.</p>
<p>For a given task, docTR provides a Predictor, which is composed of 2 components:</p>
<ul class="simple">
<li><p>PreProcessor: a module in charge of making inputs directly usable by the deep learning model.</p></li>
<li><p>Model: a deep learning model, implemented with all supported deep learning backends (TensorFlow &amp; PyTorch) along with its specific post-processor to make outputs structured and reusable.</p></li>
</ul>
<section id="text-detection">
<h2>Text Detection<a class="headerlink" href="#text-detection" title="Permalink to this headline">¶</a></h2>
<p>The task consists of localizing textual elements in a given image.
While those text elements can represent many things, in docTR, we will consider uninterrupted character sequences (words). Additionally, the localization can take several forms: from straight bounding boxes (delimited by the 2D coordinates of the top-left and bottom-right corner), to polygons, or binary segmentation (flagging which pixels belong to this element, and which don’t).</p>
<section id="available-architectures">
<h3>Available architectures<a class="headerlink" href="#available-architectures" title="Permalink to this headline">¶</a></h3>
<p>The following architectures are currently supported:</p>
<ul class="simple">
<li><p><a class="reference external" href="models.html#doctr.models.detection.linknet16">linknet16</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.detection.db_resnet50">db_resnet50</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.detection.db_mobilenet_v3_large">db_mobilenet_v3_large</a></p></li>
</ul>
<p>For a comprehensive comparison, we have compiled a detailed benchmark on publicly available datasets:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 26%" />
<col style="width: 13%" />
<col style="width: 11%" />
<col style="width: 9%" />
<col style="width: 12%" />
<col style="width: 9%" />
<col style="width: 12%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" colspan="3"></th>
<th class="head" colspan="2"><p>FUNSD</p></th>
<th class="head" colspan="2"><p>CORD</p></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Architecture</strong></p></td>
<td><p><strong>Input shape</strong></p></td>
<td><p><strong># params</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>FPS</strong></p></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td><p>25.2 M</p></td>
<td><p>82.14</p></td>
<td><p>87.64</p></td>
<td><p>92.49</p></td>
<td><p>89.66</p></td>
<td><p>2.1</p></td>
</tr>
<tr class="row-even"><td><p>db_mobilenet_v3_large</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td><p>4.2 M</p></td>
<td><p>79.35</p></td>
<td><p>84.03</p></td>
<td><p>81.14</p></td>
<td><p>66.85</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>All text detection models above have been evaluated using both the training and evaluation sets of FUNSD and CORD (cf. <a class="reference internal" href="datasets.html#datasets"><span class="std std-ref">Available Datasets</span></a>).
Explanations about the metrics being used are available in <a class="reference internal" href="utils.html#metrics"><span class="std std-ref">Task evaluation</span></a>.</p>
<p><em>Disclaimer: both FUNSD subsets combined have 199 pages which might not be representative enough of the model capabilities</em></p>
<p>FPS (Frames per second) is computed after a warmup phase of 100 tensors (where the batch size is 1), by measuring the average number of processed tensors per second over 1000 samples. Those results were obtained on a <a class="reference external" href="https://aws.amazon.com/ec2/instance-types/c5/">c5.x12large</a> AWS instance (CPU Xeon Platinum 8275L).</p>
</section>
<section id="detection-predictors">
<h3>Detection predictors<a class="headerlink" href="#detection-predictors" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="models.html#doctr.models.detection.detection_predictor">detection_predictor</a> wraps your detection model to make it easily useable with your favorite deep learning framework seamlessly.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">detection_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictor</span> <span class="o">=</span> <span class="n">detection_predictor</span><span class="p">(</span><span class="s1">&#39;db_resnet50&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dummy_img</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">800</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">dummy_img</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="text-recognition">
<h2>Text Recognition<a class="headerlink" href="#text-recognition" title="Permalink to this headline">¶</a></h2>
<p>The task consists of transcribing the character sequence in a given image.</p>
<section id="id1">
<h3>Available architectures<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The following architectures are currently supported:</p>
<ul class="simple">
<li><p><a class="reference external" href="models.html#doctr.models.recognition.crnn_vgg16_bn">crnn_vgg16_bn</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.recognition.crnn_mobilenet_v3_small">crnn_mobilenet_v3_small</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.recognition.crnn_mobilenet_v3_large">crnn_mobilenet_v3_large</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.recognition.sar_resnet31">sar_resnet31</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.recognition.master">master</a></p></li>
</ul>
<p>For a comprehensive comparison, we have compiled a detailed benchmark on publicly available datasets:</p>
<table class="docutils align-default" id="id5">
<caption><span class="caption-text">Text recognition model zoo</span><a class="headerlink" href="#id5" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Input shape</p></th>
<th class="head"><p># params</p></th>
<th class="head"><p>FUNSD</p></th>
<th class="head"><p>CORD</p></th>
<th class="head"><p>FPS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>crnn_vgg16_bn</p></td>
<td><p>(32, 128, 3)</p></td>
<td><p>15.8M</p></td>
<td><p>87.15</p></td>
<td><p>92.92</p></td>
<td><p>12.8</p></td>
</tr>
<tr class="row-odd"><td><p>crnn_mobilenet_v3_small</p></td>
<td><p>(32, 128, 3)</p></td>
<td><p>2.1M</p></td>
<td><p>86.21</p></td>
<td><p>90.56</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>crnn_mobilenet_v3_large</p></td>
<td><p>(32, 128, 3)</p></td>
<td><p>4.5M</p></td>
<td><p>86.95</p></td>
<td><p>92.03</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sar_resnet31</p></td>
<td><p>(32, 128, 3)</p></td>
<td><p>56.2M</p></td>
<td><p><strong>87.70</strong></p></td>
<td><p><strong>93.41</strong></p></td>
<td><p>2.7</p></td>
</tr>
<tr class="row-even"><td><p>master</p></td>
<td><p>(32, 128, 3)</p></td>
<td><p>67.7M</p></td>
<td><p>87.62</p></td>
<td><p>93.27</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>All text recognition models above have been evaluated using both the training and evaluation sets of FUNSD and CORD (cf. <a class="reference internal" href="datasets.html#datasets"><span class="std std-ref">Available Datasets</span></a>).
Explanations about the metric being used (exact match) are available in <a class="reference internal" href="utils.html#metrics"><span class="std std-ref">Task evaluation</span></a>.</p>
<p>While most of our recognition models were trained on our french vocab (cf. <a class="reference internal" href="datasets.html#vocabs"><span class="std std-ref">Supported Vocabs</span></a>), you can easily access the vocab of any model as follows:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">recognition_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictor</span> <span class="o">=</span> <span class="n">recognition_predictor</span><span class="p">(</span><span class="s1">&#39;crnn_vgg16_bn&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">predictor</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;vocab&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><em>Disclaimer: both FUNSD subsets combine have 30595 word-level crops which might not be representative enough of the model capabilities</em></p>
<p>FPS (Frames per second) is computed after a warmup phase of 100 tensors (where the batch size is 1), by measuring the average number of processed tensors per second over 1000 samples. Those results were obtained on a <a class="reference external" href="https://aws.amazon.com/ec2/instance-types/c5/">c5.x12large</a> AWS instance (CPU Xeon Platinum 8275L).</p>
</section>
<section id="recognition-predictors">
<h3>Recognition predictors<a class="headerlink" href="#recognition-predictors" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="models.html#doctr.models.recognition.recognition_predictor">recognition_predictor</a> wraps your recognition model to make it easily useable with your favorite deep learning framework seamlessly.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">recognition_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictor</span> <span class="o">=</span> <span class="n">recognition_predictor</span><span class="p">(</span><span class="s1">&#39;crnn_vgg16_bn&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dummy_img</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">dummy_img</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="end-to-end-ocr">
<h2>End-to-End OCR<a class="headerlink" href="#end-to-end-ocr" title="Permalink to this headline">¶</a></h2>
<p>The task consists of both localizing and transcribing textual elements in a given image.</p>
<section id="id3">
<h3>Available architectures<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>You can use any combination of detection and recognition models supporte by docTR.</p>
<p>For a comprehensive comparison, we have compiled a detailed benchmark on publicly available datasets:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 36%" />
<col style="width: 11%" />
<col style="width: 13%" />
<col style="width: 8%" />
<col style="width: 11%" />
<col style="width: 13%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head" colspan="3"><p>FUNSD</p></th>
<th class="head" colspan="3"><p>CORD</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Architecture</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>FPS</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>FPS</strong></p></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + crnn_vgg16_bn</p></td>
<td><p>71.00</p></td>
<td><p>76.02</p></td>
<td><p>0.85</p></td>
<td><p>83.87</p></td>
<td><p>81.34</p></td>
<td><p>1.6</p></td>
</tr>
<tr class="row-even"><td><p>db_resnet50 + master</p></td>
<td><p>71.03</p></td>
<td><p>76.06</p></td>
<td></td>
<td><p>84.49</p></td>
<td><p>81.94</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + sar_resnet31</p></td>
<td><p>71.25</p></td>
<td><p>76.29</p></td>
<td><p>0.27</p></td>
<td><p>84.50</p></td>
<td><p><strong>81.96</strong></p></td>
<td><p>0.83</p></td>
</tr>
<tr class="row-even"><td><p>db_resnet50 + crnn_mobilenet_v3_small</p></td>
<td><p>69.85</p></td>
<td><p>74.80</p></td>
<td></td>
<td><p>80.85</p></td>
<td><p>78.42</p></td>
<td><p>0.83</p></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + crnn_mobilenet_v3_large</p></td>
<td><p>70.57</p></td>
<td><p>75.57</p></td>
<td></td>
<td><p>82.57</p></td>
<td><p>80.08</p></td>
<td><p>0.83</p></td>
</tr>
<tr class="row-even"><td><p>db_mobilenet_v3_large + crnn_vgg16_bn</p></td>
<td><p>67.73</p></td>
<td><p>71.73</p></td>
<td></td>
<td><p>71.65</p></td>
<td><p>59.03</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Gvision text detection</p></td>
<td><p>59.50</p></td>
<td><p>62.50</p></td>
<td></td>
<td><p>75.30</p></td>
<td><p>70.00</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Gvision doc. text detection</p></td>
<td><p>64.00</p></td>
<td><p>53.30</p></td>
<td></td>
<td><p>68.90</p></td>
<td><p>61.10</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AWS textract</p></td>
<td><p><strong>78.10</strong></p></td>
<td><p><strong>83.00</strong></p></td>
<td></td>
<td><p><strong>87.50</strong></p></td>
<td><p>66.00</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>All OCR models above have been evaluated using both the training and evaluation sets of FUNSD and CORD (cf. <a class="reference internal" href="datasets.html#datasets"><span class="std std-ref">Available Datasets</span></a>).
Explanations about the metrics being used are available in <a class="reference internal" href="utils.html#metrics"><span class="std std-ref">Task evaluation</span></a>.</p>
<p><em>Disclaimer: both FUNSD subsets combine have 199 pages which might not be representative enough of the model capabilities</em></p>
<p>FPS (Frames per second) is computed after a warmup phase of 100 tensors (where the batch size is 1), by measuring the average number of processed frames per second over 1000 samples. Those results were obtained on a <a class="reference external" href="https://aws.amazon.com/ec2/instance-types/c5/">c5.x12large</a> AWS instance (CPU Xeon Platinum 8275L).</p>
<p>Since you may be looking for specific use cases, we also performed this benchmark on private datasets with various document types below. Unfortunately, we are not able to share those at the moment since they contain sensitive information.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 6%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head" colspan="2"><p>Receipts</p></th>
<th class="head" colspan="2"><p>Invoices</p></th>
<th class="head" colspan="2"><p>IDs</p></th>
<th class="head" colspan="2"><p>US Tax Forms</p></th>
<th class="head" colspan="2"><p>Resumes</p></th>
<th class="head" colspan="2"><p>Road Fines</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Architecture</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + crnn_vgg16_bn (ours)</p></td>
<td><p>78.70</p></td>
<td><p>81.12</p></td>
<td><p>65.80</p></td>
<td><p>70.70</p></td>
<td><p>50.25</p></td>
<td><p>51.78</p></td>
<td><p>79.08</p></td>
<td><p>92.83</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>db_resnet50 + master (ours)</p></td>
<td><p><strong>79.00</strong></p></td>
<td><p><strong>81.42</strong></p></td>
<td><p>65.57</p></td>
<td><p>69.86</p></td>
<td><p>51.34</p></td>
<td><p>52.90</p></td>
<td><p>78.86</p></td>
<td><p>92.57</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + sar_resnet31 (ours)</p></td>
<td><p>78.94</p></td>
<td><p>81.37</p></td>
<td><p>65.89</p></td>
<td><p><strong>70.79</strong></p></td>
<td><p><strong>51.78</strong></p></td>
<td><p><strong>53.35</strong></p></td>
<td><p>79.04</p></td>
<td><p>92.78</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>db_resnet50 + crnn_mobilenet_v3_small (ours)</p></td>
<td><p>76.81</p></td>
<td><p>79.15</p></td>
<td><p>64.89</p></td>
<td><p>69.61</p></td>
<td><p>45.03</p></td>
<td><p>46.38</p></td>
<td><p>78.96</p></td>
<td><p>92.11</p></td>
<td><p>85.91</p></td>
<td><p>87.20</p></td>
<td><p>84.85</p></td>
<td><p>85.86</p></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + crnn_mobilenet_v3_large (ours)</p></td>
<td><p>78.01</p></td>
<td><p>80.39</p></td>
<td><p>65.36</p></td>
<td><p>70.11</p></td>
<td><p>48.00</p></td>
<td><p>49.43</p></td>
<td><p>79.39</p></td>
<td><p>92.62</p></td>
<td><p>87.68</p></td>
<td><p>89.00</p></td>
<td><p>85.65</p></td>
<td><p>86.67</p></td>
</tr>
<tr class="row-even"><td><p>db_mobilenet_v3_large + crnn_vgg16_bn (ours)</p></td>
<td><p>78.36</p></td>
<td><p>74.93</p></td>
<td><p>63.04</p></td>
<td><p>68.41</p></td>
<td><p>39.36</p></td>
<td><p>41.75</p></td>
<td><p>72.14</p></td>
<td><p>89.97</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Gvision doc. text detection</p></td>
<td><p>68.91</p></td>
<td><p>59.89</p></td>
<td><p>63.20</p></td>
<td><p>52.85</p></td>
<td><p>43.70</p></td>
<td><p>29.21</p></td>
<td><p>69.79</p></td>
<td><p>65.68</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AWS textract</p></td>
<td><p>75.77</p></td>
<td><p>77.70</p></td>
<td><p><strong>70.47</strong></p></td>
<td><p>69.13</p></td>
<td><p>46.39</p></td>
<td><p>43.32</p></td>
<td><p><strong>84.31</strong></p></td>
<td><p><strong>98.11</strong></p></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="two-stage-approaches">
<h3>Two-stage approaches<a class="headerlink" href="#two-stage-approaches" title="Permalink to this headline">¶</a></h3>
<p>Those architectures involve one stage of text detection, and one stage of text recognition. The text detection will be used to produces cropped images that will be passed into the text recognition block. Everything is wrapped up with <a class="reference external" href="models.html#doctr.models.ocr_predictor">ocr_predictor</a>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">ocr_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ocr_predictor</span><span class="p">(</span><span class="s1">&#39;db_resnet50&#39;</span><span class="p">,</span> <span class="s1">&#39;crnn_vgg16_bn&#39;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_page</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">800</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">input_page</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="what-should-i-do-with-the-output">
<h3>What should I do with the output?<a class="headerlink" href="#what-should-i-do-with-the-output" title="Permalink to this headline">¶</a></h3>
<p>The ocr_predictor returns a <cite>Document</cite> object with a nested structure (with <cite>Page</cite>, <cite>Block</cite>, <cite>Line</cite>, <cite>Word</cite>, <cite>Artefact</cite>).
To get a better understanding of our document model, check our <a class="reference internal" href="io.html#document-structure"><span class="std std-ref">Document structure</span></a> section</p>
<p>Here is a typical <cite>Document</cite> layout:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">Document</span><span class="p">(</span>
  <span class="p">(</span><span class="n">pages</span><span class="p">):</span> <span class="p">[</span><span class="n">Page</span><span class="p">(</span>
    <span class="n">dimensions</span><span class="o">=</span><span class="p">(</span><span class="mi">340</span><span class="p">,</span> <span class="mi">600</span><span class="p">)</span>
    <span class="p">(</span><span class="n">blocks</span><span class="p">):</span> <span class="p">[</span><span class="n">Block</span><span class="p">(</span>
      <span class="p">(</span><span class="n">lines</span><span class="p">):</span> <span class="p">[</span><span class="n">Line</span><span class="p">(</span>
        <span class="p">(</span><span class="n">words</span><span class="p">):</span> <span class="p">[</span>
          <span class="n">Word</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s1">&#39;No.&#39;</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.91</span><span class="p">),</span>
          <span class="n">Word</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s1">&#39;RECEIPT&#39;</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">),</span>
          <span class="n">Word</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s1">&#39;DATE&#39;</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.96</span><span class="p">),</span>
        <span class="p">]</span>
      <span class="p">)]</span>
      <span class="p">(</span><span class="n">artefacts</span><span class="p">):</span> <span class="p">[]</span>
    <span class="p">)]</span>
  <span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You can also export them as a nested dict, more appropriate for JSON format:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">json_output</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">export</span><span class="p">()</span>
</pre></div>
</div>
<p>For reference, here is the JSON export for the same <cite>Document</cite> as above:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s1">&#39;pages&#39;</span><span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span>
          <span class="s1">&#39;page_idx&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
          <span class="s1">&#39;dimensions&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">340</span><span class="p">,</span> <span class="mi">600</span><span class="p">),</span>
          <span class="s1">&#39;orientation&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
          <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
          <span class="s1">&#39;blocks&#39;</span><span class="p">:</span> <span class="p">[</span>
              <span class="p">{</span>
                  <span class="s1">&#39;geometry&#39;</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.1357421875</span><span class="p">,</span> <span class="mf">0.0361328125</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.8564453125</span><span class="p">,</span> <span class="mf">0.8603515625</span><span class="p">)),</span>
                  <span class="s1">&#39;lines&#39;</span><span class="p">:</span> <span class="p">[</span>
                      <span class="p">{</span>
                          <span class="s1">&#39;geometry&#39;</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.1357421875</span><span class="p">,</span> <span class="mf">0.0361328125</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.8564453125</span><span class="p">,</span> <span class="mf">0.8603515625</span><span class="p">)),</span>
                          <span class="s1">&#39;words&#39;</span><span class="p">:</span> <span class="p">[</span>
                              <span class="p">{</span>
                                  <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="s1">&#39;No.&#39;</span><span class="p">,</span>
                                  <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="mf">0.914085328578949</span><span class="p">,</span>
                                  <span class="s1">&#39;geometry&#39;</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.5478515625</span><span class="p">,</span> <span class="mf">0.06640625</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5810546875</span><span class="p">,</span> <span class="mf">0.0966796875</span><span class="p">))</span>
                              <span class="p">},</span>
                              <span class="p">{</span>
                                  <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="s1">&#39;RECEIPT&#39;</span><span class="p">,</span>
                                  <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="mf">0.9949972033500671</span><span class="p">,</span>
                                  <span class="s1">&#39;geometry&#39;</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.1357421875</span><span class="p">,</span> <span class="mf">0.0361328125</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.51171875</span><span class="p">,</span> <span class="mf">0.1630859375</span><span class="p">))</span>
                              <span class="p">},</span>
                              <span class="p">{</span>
                                  <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="s1">&#39;DATE&#39;</span><span class="p">,</span>
                                  <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="mf">0.9578408598899841</span><span class="p">,</span>
                                  <span class="s1">&#39;geometry&#39;</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.1396484375</span><span class="p">,</span> <span class="mf">0.3232421875</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.185546875</span><span class="p">,</span> <span class="mf">0.3515625</span><span class="p">))</span>
                              <span class="p">}</span>
                          <span class="p">]</span>
                      <span class="p">}</span>
                  <span class="p">],</span>
                  <span class="s1">&#39;artefacts&#39;</span><span class="p">:</span> <span class="p">[]</span>
              <span class="p">}</span>
          <span class="p">]</span>
      <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="using_model_export.html" class="btn btn-neutral float-right" title="Preparing your model for inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="installing.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, Mindee

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'G-40DVRMX8T4', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>